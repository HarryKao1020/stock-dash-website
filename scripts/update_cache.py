#!/usr/bin/env python3
"""
å®šæœŸæ›´æ–° FinLab å¿«å–è³‡æ–™ï¼ˆParquet æ ¼å¼ï¼‰

ç”¨é€”ï¼š
- ç”± cron job å®šæœŸåŸ·è¡Œï¼ˆå»ºè­°ï¼šäº¤æ˜“æ—¥æ—©ä¸Š 7:30ï¼‰
- é å…ˆä¸‹è¼‰ä¸¦å¿«å–è³‡æ–™ï¼Œä½¿ç”¨è€…å­˜å–æ™‚ç›´æ¥è®€å–å¿«å–
- æ­é… Parquet æ ¼å¼ï¼Œå¤§å¹…æ¸›å°‘å¿«å–æª”æ¡ˆå¤§å°ï¼ˆ~813 MB â†’ ~80-160 MBï¼‰

åŸ·è¡Œæ–¹å¼ï¼š
  python scripts/update_cache.py

Docker ç’°å¢ƒï¼š
  docker-compose exec web python scripts/update_cache.py

Cron è¨­å®šç¯„ä¾‹ï¼ˆäº¤æ˜“æ—¥æ—©ä¸Š 7:30ï¼‰ï¼š
  30 7 * * 1-5 cd /path/to/project && docker-compose exec -T web python scripts/update_cache.py

  æˆ–æœ¬åœ°ç’°å¢ƒï¼š
  30 7 * * 1-5 cd /path/to/project && python3 scripts/update_cache.py
"""

import sys
import logging
from pathlib import Path
from datetime import datetime

# åŠ å…¥å°ˆæ¡ˆè·¯å¾‘
PROJECT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_DIR))

from data.finlab_data import finlab_data

# è¨­å®šæ—¥èªŒç›®éŒ„å’Œæª”æ¡ˆ
LOGS_DIR = PROJECT_DIR / "logs"
LOGS_DIR.mkdir(exist_ok=True)

# ä½¿ç”¨ç•¶å¤©æ—¥æœŸä½œç‚ºæ—¥èªŒæª”å (æ ¼å¼: YYYYMMDDHHMM_log.txt)
LOG_FILENAME = datetime.now().strftime("%Y%m%d%H%M_log.txt")
LOG_FILE = LOGS_DIR / LOG_FILENAME

# è¨­å®šæ—¥èªŒæ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)  # åŒæ™‚è¼¸å‡ºåˆ°çµ‚ç«¯
    ]
)

def cleanup_old_logs(days=7):
    """æ¸…ç† N å¤©å‰çš„èˆŠæ—¥èªŒæª”æ¡ˆ"""
    from datetime import timedelta

    cutoff_time = datetime.now() - timedelta(days=days)
    deleted_count = 0

    for log_file in LOGS_DIR.glob("*_log.txt"):
        if log_file.is_file():
            file_mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
            if file_mtime < cutoff_time:
                try:
                    log_file.unlink()
                    logging.info(f"ğŸ—‘ï¸  å·²åˆªé™¤èˆŠæ—¥èªŒ: {log_file.name}")
                    deleted_count += 1
                except Exception as e:
                    logging.warning(f"âš ï¸  ç„¡æ³•åˆªé™¤ {log_file.name}: {e}")

    if deleted_count > 0:
        logging.info(f"âœ… å…±æ¸…ç† {deleted_count} å€‹èˆŠæ—¥èªŒæª”æ¡ˆï¼ˆè¶…é {days} å¤©ï¼‰")
    else:
        logging.info(f"â„¹ï¸  ç„¡éœ€æ¸…ç†ï¼ˆæ²’æœ‰è¶…é {days} å¤©çš„æ—¥èªŒï¼‰")

def main():
    """åŸ·è¡Œå¿«å–æ›´æ–°"""
    start_time = datetime.now()

    logging.info("=" * 70)
    logging.info("ğŸ”„ FinLab å¿«å–æ›´æ–°é–‹å§‹")
    logging.info(f"â° é–‹å§‹æ™‚é–“: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logging.info(f"ğŸ“… æ›´æ–°æ—¥æœŸ: {start_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %A')}")
    logging.info(f"ğŸ“ æ—¥èªŒæª”æ¡ˆ: {LOG_FILENAME}")
    logging.info("=" * 70)

    # æ¸…ç† 7 å¤©å‰çš„èˆŠæ—¥èªŒ
    logging.info("ğŸ§¹ æª¢æŸ¥ä¸¦æ¸…ç†èˆŠæ—¥èªŒ...")
    cleanup_old_logs(days=7)

    try:
        # ä½¿ç”¨ global singletonï¼ˆèˆ‡æ‡‰ç”¨ç¨‹å¼å…±ç”¨åŒä¸€å¯¦ä¾‹ï¼‰
        finlab = finlab_data

        # å¼·åˆ¶åˆ·æ–°æ‰€æœ‰å¿«å–
        logging.info("ğŸ—‘ï¸  æ¸…é™¤èˆŠå¿«å–...")
        finlab.refresh()

        logging.info("ğŸ“¥ é‡æ–°ä¸‹è¼‰è³‡æ–™ï¼ˆParquet æ ¼å¼ï¼‰...")

        # ä¸»å‹•è§¸ç™¼æ‰€æœ‰å¸¸ç”¨è³‡æ–™çš„è¼‰å…¥ï¼ˆåˆ©ç”¨ lazy loading æ©Ÿåˆ¶ï¼‰
        data_items = [
            # OHLCV è³‡æ–™
            ("æ”¶ç›¤åƒ¹", lambda: finlab.close),
            ("é–‹ç›¤åƒ¹", lambda: finlab.open),
            ("æœ€é«˜åƒ¹", lambda: finlab.high),
            ("æœ€ä½åƒ¹", lambda: finlab.low),
            ("æˆäº¤é‡", lambda: finlab.volume),
            ("æˆäº¤é‡‘é¡", lambda: finlab.amount),

            # èè³‡è³‡æ–™
            ("èè³‡é¤˜é¡", lambda: finlab.margin_balance),
            ("èè³‡ç¸½é¤˜é¡", lambda: finlab.margin_total),
            ("èè³‡ç¶­æŒç‡", lambda: finlab.margin_maintenance_ratio),
            ("å¤§ç›¤æŒ‡æ•¸", lambda: finlab.benchmark),

            # ä¸–ç•ŒæŒ‡æ•¸
            ("ä¸–ç•ŒæŒ‡æ•¸é–‹ç›¤", lambda: finlab.world_index_open),
            ("ä¸–ç•ŒæŒ‡æ•¸æ”¶ç›¤", lambda: finlab.world_index_close),
            ("ä¸–ç•ŒæŒ‡æ•¸æœ€é«˜", lambda: finlab.world_index_high),
            ("ä¸–ç•ŒæŒ‡æ•¸æœ€ä½", lambda: finlab.world_index_low),
            ("ä¸–ç•ŒæŒ‡æ•¸æˆäº¤é‡", lambda: finlab.world_index_vol),

            # è‚¡ç¥¨ç¯©é¸
            ("è™•ç½®è‚¡éæ¿¾", lambda: finlab.disposal_stock),
            ("è­¦ç¤ºè‚¡éæ¿¾", lambda: finlab.noticed_stock),

            # ç‡Ÿæ”¶è³‡æ–™
            ("ç•¶æœˆç‡Ÿæ”¶", lambda: finlab.monthly_revenue),
            ("ç‡Ÿæ”¶YoY", lambda: finlab.revenue_yoy),
            ("ç‡Ÿæ”¶MoM", lambda: finlab.revenue_mom),
        ]

        logging.info(f"ğŸ“Š é è¼‰ {len(data_items)} é …è³‡æ–™...")
        success_count = 0
        success_items = []
        failed_items = []

        for name, getter in data_items:
            try:
                data = getter()
                if data is not None:
                    if hasattr(data, 'shape'):
                        logging.info(f"   âœ… {name:<20} - å½¢ç‹€: {data.shape}")
                    else:
                        logging.info(f"   âœ… {name:<20} - å·²è¼‰å…¥")
                    success_count += 1
                    success_items.append(name)
                else:
                    logging.warning(f"   âš ï¸  {name:<20} - è³‡æ–™ç‚º None")
                    failed_items.append(name)
            except Exception as e:
                logging.error(f"   âŒ {name:<20} - éŒ¯èª¤: {str(e)[:50]}")
                failed_items.append(name)

        # è¨ˆç®—åŸ·è¡Œæ™‚é–“
        end_time = datetime.now()
        elapsed = (end_time - start_time).total_seconds()

        # æª¢æŸ¥å¿«å–ç›®éŒ„å¤§å°
        cache_dir = PROJECT_DIR / "cache"
        cache_size = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())
        cache_size_mb = cache_size / (1024 * 1024)

        logging.info("=" * 70)
        logging.info("âœ… å¿«å–æ›´æ–°å®Œæˆï¼")
        logging.info(f"ğŸ“Š æˆåŠŸ: {success_count}/{len(data_items)}")

        if success_items:
            logging.info(f"âœ… æˆåŠŸæ›´æ–°é …ç›®:")
            for item in success_items:
                logging.info(f"   - {item}")

        if failed_items:
            logging.warning(f"âš ï¸  å¤±æ•—é …ç›®: {', '.join(failed_items)}")

        logging.info(f"ğŸ’¾ å¿«å–å¤§å°: {cache_size_mb:.1f} MB")
        logging.info(f"â±ï¸  åŸ·è¡Œæ™‚é–“: {elapsed:.1f} ç§’")
        logging.info(f"â° å®Œæˆæ™‚é–“: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logging.info("=" * 70)

        return 0 if success_count == len(data_items) else 1

    except Exception as e:
        end_time = datetime.now()
        elapsed = (end_time - start_time).total_seconds()

        logging.error("=" * 70)
        logging.error(f"âŒ å¿«å–æ›´æ–°å¤±æ•—: {e}")
        logging.error(f"â±ï¸  åŸ·è¡Œæ™‚é–“: {elapsed:.1f} ç§’")
        logging.error("=" * 70)
        import traceback
        logging.error(traceback.format_exc())
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
